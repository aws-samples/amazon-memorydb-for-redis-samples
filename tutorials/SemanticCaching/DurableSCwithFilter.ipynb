{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c2eb89b-3ca2-41f3-b4be-87b2044ba545",
   "metadata": {},
   "source": [
    "## 1. Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5243cf73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install valkey\n",
    "!{sys.executable} -m pip install langchain\n",
    "!{sys.executable} -m pip install langchain_aws\n",
    "!{sys.executable} -m pip install gradio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e1675d-5b2b-4775-8e6c-770e7aa4fdc0",
   "metadata": {},
   "source": [
    "## 2 Add Amazon MemoryDB endpoint to the environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f02c7ad-c98c-45bb-8ba3-be822ac0d3ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%env MEMORYDB_HOST=cluster-endpoint\n",
    "%env MEMORYDB_PORT=6379"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f270ae51-923d-4b2e-9ade-589eeb89169e",
   "metadata": {},
   "source": [
    "## 3. Setup logging and import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0523a51e-ebff-43d4-8bfd-f65ebe7a1840",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "import traceback\n",
    "import numpy as np\n",
    "import os\n",
    "import redis\n",
    "from redis.cluster import RedisCluster as MemoryDB\n",
    "from redis.commands.search.field import TagField, VectorField, TextField\n",
    "from redis.commands.search.indexDefinition import IndexDefinition, IndexType\n",
    "from redis.commands.search.query import Query\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain_aws.embeddings import BedrockEmbeddings\n",
    "import gradio as gr\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a20535ef-c1c8-4e03-a18f-400e68f6462e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "stdout_handler.setLevel(logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(stdout_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988f0b3d-b4c4-4c6c-a9cf-ba1cf075cfaa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Connect to MemoryDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dd8c991-8a40-4e5d-ba4c-7a91a067b770",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.1 ms, sys: 3.95 ms, total: 53.1 ms\n",
      "Wall time: 61.7 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "memorydb_host = os.environ.get(\"MEMORYDB_HOST\", \"localhost\")\n",
    "memorydb_port = os.environ.get(\"MEMORYDB_PORT\", 6379)\n",
    "# print(f\"MemoryDB Url = {memorydb_host}:{memorydb_port}\")\n",
    "rc = MemoryDB(host=memorydb_host, port=memorydb_port, ssl=False, decode_responses=False, ssl_cert_reqs=\"none\")\n",
    "rc.ping()\n",
    "#rc.flushall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4076ae70-6772-4fd7-8dec-26968e6f7d96",
   "metadata": {},
   "source": [
    " ## 5. Setup index and model constants "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef7a7091-40f2-4ca3-95b5-ba0bd0ca709d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "INDEX_NAME = \"bedrock2\"\n",
    "DOC_PREFIX = \"doc:\"\n",
    "#knowledge_base_id = os.getenv(\"KNOWLEDGE_BASE_ID\")\n",
    "model_id = 'anthropic.claude-v2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5aea4998-4477-4310-9489-799338567797",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    }
   ],
   "source": [
    "bedrock_client = boto3.client('bedrock-runtime', region_name=\"us-east-1\")\n",
    "bedrock_agent_runtime = boto3.client('bedrock-agent-runtime', region_name=\"us-east-1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea4680b-e76e-4636-91b7-2cee2fbf5c4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Create index in MemoryDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3500151d-6dbb-44ba-aa27-95102a4fd102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def setup_index():\n",
    "    \"\"\"Create or check the existence of an index.\"\"\"\n",
    "    logger.info(f\"Creating index {INDEX_NAME}\")\n",
    "    try:\n",
    "        rc.ft(INDEX_NAME).info()\n",
    "        logger.info(\"Index already exists!\")\n",
    "    except:\n",
    "        schema = (\n",
    "            TextField(\"question\"),\n",
    "            TextField(\"answer\"),\n",
    "            TagField(\"tag\"),\n",
    "            TagField(\"country\"),\n",
    "            VectorField(\"vector\", \"HNSW\", {\n",
    "                \"TYPE\": \"FLOAT32\",\n",
    "                \"DIM\": 1536,\n",
    "                \"DISTANCE_METRIC\": \"COSINE\",\n",
    "            }),\n",
    "        )\n",
    "        definition = IndexDefinition(prefix=[DOC_PREFIX], index_type=IndexType.HASH)\n",
    "        rc.ft(INDEX_NAME).create_index(fields=schema, definition=definition)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e051af65-1228-4a21-a6a1-b2cbe3fd60bf",
   "metadata": {},
   "source": [
    "## 7. Convert to embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a856b57-f23c-469f-a962-7d5263ae0419",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_embedding(text_content):\n",
    "    \"\"\"Generate embeddings for a given piece of text using AWS Bedrock service.\"\"\"\n",
    "    try:\n",
    "        body_content = json.dumps({\"inputText\": text_content})\n",
    "        response = bedrock_client.invoke_model(\n",
    "            body=body_content,\n",
    "            contentType=\"application/json\",\n",
    "            accept=\"*/*\",\n",
    "            modelId=\"amazon.titan-embed-text-v1\"\n",
    "        )\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        return response_body.get('embedding')\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating embedding: {e}\")\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a656a1b-5855-46cc-abbb-96736cb1873e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 8. Lookup from Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f205036-ad87-4462-b323-43f66a16c588",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lookup_cache_range(user_question, user_question_embedding, country=None):\n",
    "    \"\"\"Search the cache for a similar question with optional country-based filtering.\"\"\"\n",
    "    setup_index()\n",
    "    start_time = time.time()\n",
    "    question_embedding = np.array(user_question_embedding, dtype=np.float32).tobytes()\n",
    "\n",
    "    # Add tag-based filtering for country if provided\n",
    "    country_filter = f'@country:{{{country}}} ' if country else \"\"\n",
    "    q = Query(f'{country_filter}@vector:[VECTOR_RANGE $radius $vec]=>{{$YIELD_DISTANCE_AS: score}}').paging(0, 1).dialect(2).return_fields(\"question\", \"answer\", \"score\")\n",
    "\n",
    "    query_params = {\n",
    "        \"radius\": 0.2,\n",
    "        \"vec\": question_embedding\n",
    "    }\n",
    "\n",
    "    results = rc.ft(INDEX_NAME).search(q, query_params).docs\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    if results:\n",
    "        logger.info(\"Cache hit found!\")\n",
    "        logger.info(f\"Cache query executed in {execution_time*1000:.4f} milliseconds\")\n",
    "        return results[0].__dict__\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e669884-68ef-4f8c-9f29-d04d75c540bd",
   "metadata": {},
   "source": [
    "## 9.Add to Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c6accc9-7ca0-4255-aeb5-acad4636bb90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_to_cache(user_question, user_question_embedding, answer, country=None):\n",
    "    \"\"\"Add question, answer, and optional country tag to the cache.\"\"\"\n",
    "    question_embedding = np.array(user_question_embedding, dtype=np.float32).tobytes()\n",
    "    key = f'{DOC_PREFIX}{hash(user_question) % 2**sys.hash_info.width}'\n",
    "\n",
    "    # Set the fields to be added to the cache, including the optional country tag\n",
    "    cache_data = {\n",
    "        \"vector\": question_embedding,\n",
    "        \"question\": user_question,\n",
    "        \"answer\": answer,\n",
    "        \"tag\": \"amazon.titan-embed-text-v1\"\n",
    "    }\n",
    "\n",
    "    # Add the country tag if provided\n",
    "    if country:\n",
    "        cache_data[\"country\"] = country\n",
    "\n",
    "    rc.hset(key, mapping=cache_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2045d4eb-5bfd-44ad-ace8-d9c597a8c65b",
   "metadata": {},
   "source": [
    "## 10.Initialize the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0867e01c-b4c5-4071-a7f5-6c638433a24f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_llm():\n",
    "    model_kwargs = {\n",
    "    \"temperature\": 0, \n",
    "    \"top_k\": 250, \n",
    "    \"top_p\": 1,\n",
    "    \"stop_sequences\": [\"\\\\n\\\\nHuman:\"]\n",
    "    }\n",
    "\n",
    "    # use the Anthropic Claude model\n",
    "    llm = ChatBedrock(\n",
    "        model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "        model_kwargs=model_kwargs\n",
    "        )\n",
    "\n",
    "    return llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b4937f-e5bf-4813-b3f0-e780657a292a",
   "metadata": {},
   "source": [
    "## 11.Submit a question to model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91f8e91a-6a8a-4e4d-bf3b-ef17d62d3e4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def answer_question_with_model(question, country=None):\n",
    "    llm = get_llm()\n",
    "\n",
    "    # Conditionally construct a location-specific prompt if country is provided\n",
    "    if country:\n",
    "        location_prompt = f\"This user is based in {country}, only give answers relevant to their location.\"\n",
    "        full_question = f\"{location_prompt}\\n\\n{question}\"\n",
    "    else:\n",
    "        full_question = question\n",
    "\n",
    "    try:\n",
    "        # Generate a response using the LLM with or without the location-specific prompt\n",
    "        response_text = llm.invoke(full_question)\n",
    "        return response_text.content\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions that occur during LLM prediction\n",
    "        print(f\"Error during LLM prediction: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ae3577-8df3-4e55-82c9-f166560330d8",
   "metadata": {},
   "source": [
    "## 12. Retrieve or generate answer using semantic caching without any filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7e65c23-aea6-4215-8e7a-6a56b79ae5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_question_semantic(user_question):\n",
    "    start_time = time.time()\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    embedding = get_embedding(user_question)\n",
    "    cached_answer = lookup_cache_range(user_question, embedding)\n",
    "\n",
    "    if cached_answer and cached_answer['answer']:\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        return f\"Answer from cache at {timestamp} (Execution time: {execution_time*1000:.2f} milliseconds): {cached_answer['answer']}\"\n",
    "    else:\n",
    "        answer = answer_question_with_model(user_question)\n",
    "        add_to_cache(user_question, embedding, answer)\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        return f\"Answer from model at {timestamp} (Execution time: {execution_time:.2f} seconds): {answer}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45bb0627-7de5-4fd0-bf61-387be1ab8c26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_question_with_country_filter(user_question, country):\n",
    "    \"\"\"Process the question with a country filter applied to the cache query.\"\"\"\n",
    "    start_time = time.time()\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Generate the embedding for the user's question\n",
    "    embedding = get_embedding(user_question)\n",
    "    # Perform the cache lookup with country filtering\n",
    "    cached_answer = lookup_cache_range(user_question, embedding, country=country)\n",
    "    if cached_answer and cached_answer['answer']:\n",
    "        # If a cached answer is found, return it with timing information\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        return f\"Answer from cache with country filter ({country}) at {timestamp} (Execution time: {execution_time*1000:.2f} milliseconds): {cached_answer['answer']}\"\n",
    "    else:\n",
    "        # If no cached answer is found, generate an answer using the model and add it to the cache\n",
    "        answer = answer_question_with_model(user_question, country)\n",
    "        add_to_cache(user_question, embedding, answer, country=country)\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        return f\"Answer from model at {timestamp} (Execution time: {execution_time:.2f} seconds): {answer}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead8de65-4ceb-4f7b-80f2-384320b3cc96",
   "metadata": {},
   "source": [
    "## 13 Retrieve or generate answer using semantic caching without any filters country filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34b03429-9bef-4ca1-89a9-db92a0c55cae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_question(selected_option, question, country=None):\n",
    "    if selected_option == \"Semantic Caching without any Filters\":\n",
    "        return process_question_semantic(question)\n",
    "    elif selected_option == \"Semantic Caching with Filters\":\n",
    "        return process_question_with_country_filter(question, country)\n",
    "\n",
    "def update_inputs(selected_option):\n",
    "    if selected_option == \"Semantic Caching with Filters\":\n",
    "        return gr.update(visible=True)\n",
    "    else:\n",
    "        return gr.update(visible=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89024a24-0d71-4569-81b4-5a7137afcf9e",
   "metadata": {},
   "source": [
    "## 14. Build UI interface for semantic caching demo with optional country filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8888efd-6138-4f7f-9982-c530ace5c54c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: GET https://checkip.amazonaws.com/ \"HTTP/1.1 200 \"\n",
      "Running on local URL:  http://127.0.0.1:7860\n",
      "HTTP Request: GET http://127.0.0.1:7860/startup-events \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.gradio.app/v2/tunnel-request \"HTTP/1.1 200 OK\"\n",
      "Running on public URL: https://b0503f27b8706d5046.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
      "HTTP Request: HEAD https://b0503f27b8706d5046.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://b0503f27b8706d5046.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index bedrock2\n",
      "Index already exists!\n",
      "Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "Creating index bedrock2\n",
      "Index already exists!\n",
      "Cache hit found!\n",
      "Cache query executed in 1.4286 milliseconds\n"
     ]
    }
   ],
   "source": [
    "options = [\"Semantic Caching without any Filters\", \"Semantic Caching with Filters\"]\n",
    "country_options = [\"Mexico\", \"US\", \"Canada\"]\n",
    "with gr.Blocks(title=\"Semantic Caching with MemoryDB\") as iface:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "    # 🌐 Semantic Caching with MemoryDB\n",
    "\n",
    "    Welcome to the **Semantic Caching Demo**! This interface demonstrates how semantic caching can deliver relevant answers to your questions, with options to filter responses by location.\n",
    "\n",
    "    ### 🔍 Getting Started\n",
    "    1. **Select a Search Option:**\n",
    "       - **Without Filters:** General semantic caching for broader, non-location-specific responses.\n",
    "       - **With Filters:** Restricts the search to a specific country, tailoring responses based on regional relevance.\n",
    "\n",
    "    2. **Ask Your Question:**\n",
    "       - Enter your question, and the system will check for a cached answer or generate a new response if needed.\n",
    "\n",
    "    ### 📌 Note\n",
    "    The filtering options currently available are **Mexico, US,** and **Canada**. Filtered searches will return answers best suited to the chosen country.\n",
    "\n",
    "    ---\n",
    "\n",
    "    Enjoy exploring how semantic caching optimizes responses with and without location-based filtering!\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    dropdown = gr.Dropdown(label=\"1. Select Search Option\", choices=options, value=options[0])\n",
    "    country_dropdown = gr.Dropdown(\n",
    "        label=\"2. Select Country (used as a filter for location-based responses)\",\n",
    "        choices=country_options,\n",
    "        visible=False,\n",
    "    )\n",
    "    text_input = gr.Textbox(\n",
    "        label=\"3. Enter Your Question\",\n",
    "        placeholder=\"Type your question here to get a response...\",\n",
    "    )\n",
    "    output = gr.Textbox(\n",
    "        label=\"Answer\",\n",
    "        placeholder=\"Your answer will appear here...\",\n",
    "    )\n",
    "\n",
    "    # Trigger conditional display of the country dropdown\n",
    "    dropdown.change(update_inputs, inputs=dropdown, outputs=country_dropdown)\n",
    "\n",
    "    # Process question with button click\n",
    "    submit_btn = gr.Button(\"Submit Question\")\n",
    "    submit_btn.click(\n",
    "        process_question,\n",
    "        inputs=[dropdown, text_input, country_dropdown],\n",
    "        outputs=output\n",
    "    )\n",
    "\n",
    "# Launch the interface with share and inbrowser options\n",
    "iface.launch(share=True, inbrowser=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247b0185-9734-4888-8828-55fb774d94ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
